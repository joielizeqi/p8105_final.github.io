---
title: "Cumulative impact of various factors on dog bite incidents"
output:
  html_document:
    code_folding: hide
    always_allow_html: true
    toc: true
    toc_float: true
---
```{r, message = FALSE, warning = FALSE, include = FALSE}
library(tidyverse)
library(knitr)
library(rvest)
library(broom)
library(dplyr)
library(purrr)
library(ggplot2)
library(plotly)
library(readxl)
library(kableExtra)
```

```{r clean_data, message = FALSE, include=FALSE}
# NYC zip code
url = "https://p8105.com/data/zip_codes.html"
ny_zip_codes = read_html(url) |>
  html_table() |> 
  data.frame() |> 
  janitor::clean_names() |> 
  mutate(
    borough = factor(
      county,
      levels=c("Bronx","Kings","New York", "Queens","Richmond"),
      labels=c("Bronx","Brooklyn","Manhattan","Queens",
 "Staten Island"))) 

valid_zipcodes = pull(ny_zip_codes, zip_code)

# Dog bite 
dog_bites_df = read_csv("data/Dog_Bites_Data.csv", na = c("NA", "", ".")) |> 
  janitor::clean_names() |> 
  rename_with(~ gsub("^x", "", .))


dog_bites_clean = dog_bites_df |> 
  mutate(date_of_bite = as.Date(date_of_bite, format = "%B %d %Y")) |> 
  mutate(year = format(date_of_bite, "%Y"),
         month = format(date_of_bite, "%m"),
         day = format(date_of_bite, "%d")) |> 
  mutate(year = as.factor(as.numeric(year)),
         month = factor(as.numeric(month), levels = 1:12),
         day = as.factor(as.numeric(day))
         ) |> 
  mutate(breed = str_to_lower(breed)) |> 
  mutate(breed = ifelse(grepl("mix|mxied|cross|\\s[xX]|-X|/|&|COLLILE,|yorkie poo|yorkie chon|yorkie-poo|yorkipoo|boxer beagle|shi-po|Doberman And Labrador", breed, ignore.case = TRUE), "Mixed", breed)) |> 
  mutate(breed = ifelse(grepl("bull", breed, ignore.case = TRUE), "Bull", breed)) |> 
  mutate(breed = ifelse(grepl("know|KOW|unsure|not given|certain|not sure|unc", breed, ignore.case = TRUE), "Unknown", breed)) |> 
  mutate(breed = ifelse(grepl("poo", breed, ignore.case = TRUE), "Poodle", breed)) |> 
  mutate(breed = ifelse(grepl("vizsla", breed, ignore.case = TRUE), "Vizsla", breed)) |> 
  mutate(breed = ifelse(grepl("shepher|sheherd|shepard|sheep|shpherd|sheperd", breed, ignore.case = TRUE), "Shepherd", breed)) |> 
  mutate(breed = ifelse(grepl("husky", breed, ignore.case = TRUE), "Husky", breed)) |> 
  mutate(breed = ifelse(grepl("chihuahua|chi hua", breed, ignore.case = TRUE), "Chihuahua", breed)) |>
  mutate(breed = ifelse(grepl("collie", breed, ignore.case = TRUE), "Collie", breed)) |> 
  mutate(breed = ifelse(grepl("cattle", breed, ignore.case = TRUE), "Cattle dog", breed)) |> 
  mutate(breed = ifelse(grepl("yorkie|yorkshire", breed, ignore.case = TRUE), "Yorkshire", breed)) |> 
  mutate(breed = ifelse(grepl("schnauzer", breed, ignore.case = TRUE), "Schnauzer", breed)) |> 
  mutate(breed = ifelse(grepl("coonhound", breed, ignore.case = TRUE), "Coonhound", breed)) |> 
  mutate(breed = ifelse(grepl("corgi", breed, ignore.case = TRUE), "Corgie", breed)) |> 
  mutate(breed = ifelse(grepl("dachshund", breed, ignore.case = TRUE), "Dachshund", breed)) |> 
  mutate(breed = ifelse(grepl("beagle", breed, ignore.case = TRUE), "Beagle", breed)) |> 
  mutate(breed = ifelse(grepl("west", breed, ignore.case = TRUE), "Westie", breed)) |> 
  mutate(breed = ifelse(grepl("mastiff", breed, ignore.case = TRUE), "Mastiff", breed)) |> 
  mutate(breed = ifelse(grepl("malti tzu|maltese", breed, ignore.case = TRUE), "Maltese", breed)) |> 
  mutate(breed = ifelse(grepl("shih tzu|Shichon|Shichi", breed, ignore.case = TRUE), "Shih tzu", breed)) |> 
  mutate(breed = ifelse(grepl("parson", breed, ignore.case = TRUE), "Parson", breed)) |> 
  mutate(breed = ifelse(grepl("nova", breed, ignore.case = TRUE), "Nova Scotia Duck Tolling Retriever", breed)) |> 
  mutate(breed = ifelse(grepl("Staffordshire", breed, ignore.case = TRUE), "Staffordshire", breed)) |> 
  mutate(breed = ifelse(grepl("skan mal", breed, ignore.case = TRUE), "alaskan malamute", breed)) |> 
  mutate(breed = ifelse(grepl("russell terr", breed, ignore.case = TRUE), "russell terrier", breed)) |> 
  mutate(breed = ifelse(grepl("shiba", breed, ignore.case = TRUE), "Shiba", breed)) |> 
  mutate(breed = ifelse(grepl("American Terrier", breed, ignore.case = TRUE), "American Terrier", breed)) |> 
  mutate(breed = ifelse(grepl("Golden", breed, ignore.case = TRUE), "Golden Doodle", breed)) |>
  mutate(breed = ifelse(grepl("Springer", breed, ignore.case = TRUE), "Springer", breed)) |> 
  mutate(breed = ifelse(grepl("Catahoula", breed, ignore.case = TRUE), "Catahoula", breed)) |> 
  mutate(breed = ifelse(grepl("Crested", breed, ignore.case = TRUE), "Crested", breed)) |> 
  mutate(breed = ifelse(grepl("Spaniel", breed, ignore.case = TRUE), "Spaniel", breed)) |> 
  mutate(breed = ifelse(grepl("Dandie Dinmont", breed, ignore.case = TRUE), "Dandie Dinmont", breed)) |> 
  mutate(breed = ifelse(grepl("Pug", breed, ignore.case = TRUE), "Puggle", breed)) |> 
  mutate(breed = ifelse(grepl("Potcake", breed, ignore.case = TRUE), "Potcake", breed)) |> 
  mutate(breed = ifelse(grepl("Border Terrier", breed, ignore.case = TRUE), "Border Terrier", breed)) |> 
  mutate(breed = ifelse(grepl("Blue He", breed, ignore.case = TRUE), "Blue Heeler", breed)) |> 
  mutate(breed = ifelse(grepl("Bernedoodle", breed, ignore.case = TRUE), "Bernadoodle", breed)) |> 
  mutate(breed = ifelse(grepl("ne Corso", breed, ignore.case = TRUE), "Caine Corso", breed)) |> 
  mutate(breed = ifelse(grepl("ton De Tulear", breed, ignore.case = TRUE), "Cotton De Tulear", breed)) |> 
  mutate(breed = ifelse(grepl("Daschound|Daschund|Dachshund", breed, ignore.case = TRUE), "Daschund", breed)) |> 
  mutate(breed = ifelse(grepl("Pomsk", breed, ignore.case = TRUE), "Pomski", breed)) |> 
  mutate(breed = ifelse(grepl("Miniature Labradoodle", breed, ignore.case = TRUE), "Mini Labordoodle", breed)) |> 
  mutate(breed = ifelse(grepl("Miniature Pinscher", breed, ignore.case = TRUE), "Mini Pincher", breed)) |> 
  mutate(breed = ifelse(grepl("Wheaten Terrier|wheaton", breed, ignore.case = TRUE), "Wheaton Terrier", breed)) |>
  mutate(breed = ifelse(grepl("Retriever", breed, ignore.case = TRUE), "Retreiver", breed)) |>
  mutate(breed = ifelse(grepl("Dogue De Bord", breed, ignore.case = TRUE), "Dogue De Bordeaux", breed)) |>
  mutate(breed = ifelse(grepl("Medium", breed, ignore.case = TRUE), "Medium", breed)) |>
  mutate(breed = ifelse(grepl("small", breed, ignore.case = TRUE), "small", breed)) |>
  mutate(breed = ifelse(grepl("Wolfhound", breed, ignore.case = TRUE), "Wolfhound", breed)) |>
  mutate(breed = ifelse(grepl("Terrier", breed, ignore.case = TRUE), "Terrier", breed)) |>
  mutate(breed = ifelse(grepl("small", breed, ignore.case = TRUE), "small", breed)) |>
  mutate(breed = str_to_title(breed))

# Dog license
dog_licensing_df = read_csv("data/NYC_Dog_Licensing_Dataset.csv", na = c("NA", "", ".")) |> 
  janitor::clean_names() |> 
  rename_with(~ gsub("^x", "", .))

dog_bites_filtered = dog_bites_clean %>%
  filter(gender != "U",
         zip_code %in% valid_zipcodes,
         borough != "Other") %>%
  mutate(
    gender = factor(gender),
    spay_neuter = factor(spay_neuter, levels = c(FALSE, TRUE), labels = c("Not Neutered", "Neutered")),
    borough = factor(borough),
    year = factor(year),
    month = factor(month)
  )

bites_by_group = dog_bites_filtered %>%
  count(year, month, gender, spay_neuter, borough, name = "bite_count")


```

# Variable features

The dependent variable hypothesized in the article is the number of dog bite incidents per zipcode. The predictor variables include the year and month of the incident, characteristics of the dog, and the area where the incident occurred. The year is measured as an interval-level variable, with values from 1 (corresponding to 2015) to 7 (corresponding to 2021). The area of the incident is divided into variables ranging from 1 to 5, representing the five boroughs of New York City. The gender of the dog is a binary variable, with 1 indicating female and 2 indicating male. Similarly, the sterilization status of the dog is also a binary variable, with 0 indicating unsterilized and 1 indicating sterilized.

```{r Variable, message=FALSE, warning=FALSE}
data <- data.frame(
  `Variable` = c("dog_bite_incidents", "year", "month", "gender", "spay_neuter", "borough"),
  `Range` = c(
    "$\\mathbb{N}$",
    "$\\{1, 2, 3, 4, 5, 6, 7\\}$",
    "$\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}$",
    "$\\{1, 2\\}$",
    "$\\{0, 1\\}$",
    "$\\{1, 2, 3, 4, 5\\}$"
  )
)

knitr::kable(data, format = "pipe", align = "l", caption = "Variable Implications")

```

# Negative Binomial Regression Analysis

To measure the combined effects of year, dog background characteristics (i.e., gender and whether neutered) and the Borough they belong to on the incidence of dog bites, we first conduct a negative binomial regression analysis using records of dog bite incidents in New York.

The negative binomial regression model is a type of count model designed for dependent variables that can only take non-negative integer values. From the Spatial Correlations section, the study area meets the three prerequisites for negative binomial regression: the dog bite incidents are not independent, exhibiting spatial clustering; the dependent variable shows overdispersion; and the frequency of dog bite incidents is sufficiently low. Let the dependent variable follows a negative binomial distribution. The model equation is as follows:

$$\ln ({\hat y_i}) = {\beta _0} + {\beta _1}{X_{i1}} + {\beta _2}{X_{i2}} +  \cdots {\beta _m}{X_{im}} + \xi$$

Here, ${y_i}$ represents the frequency of dog bite incidents within a year, ${\beta _m}$ is the coefficient, $X_m$ are the explanatory variables, and $\xi$ represents the heterogeneity across observations.

In negative binomial regression, where the conditional expectation $E(Y|X) = {e^{{\beta _0} + {\beta _1}{X_{i1}} + {\beta _2}{X_{i2}} +  \cdots {\beta _m}{X_{im}}}}$, the interpretation of the model's coefficients is as follows: when the explanatory variable ${X_k}$ increases by one unit, the average occurrence rate of dog bite incidents becomes ${e^{{\beta _k}}}$ times what it was originally.

$$\frac{{E(Y|{X^*})}}{{E(Y|X)}} = \frac{{{e^{{\beta _0} + {\beta _1}{X_{i1}} +  \cdots  + {\beta _k}({X_{ik}} + 1) +  \cdots  + {\beta _m}{X_{im}}}}}}{{{e^{{\beta _0} + {\beta _1}{X_{i1}} +  \cdots  + {\beta _k}{X_{ik}} +  \cdots  + {\beta _m}{X_{im}}}}}} = {e^{{\beta _k}}}$$

## Regularized Model (Lasso)

Initially, the paper employs Lasso regression on five primary variables, aiming to use regularization to reduce the number of variables in the model and thus avoid overfitting.

```{r lambda, warning=FALSE, message=FALSE}
# lasso
library(glmnet)

x = model.matrix(bite_count ~ year + month + gender + spay_neuter + borough, data = bites_by_group)
y = bites_by_group |> pull(bite_count)

cv_model = cv.glmnet(x, y, family = "poisson", alpha = 1)  # Lasso：alpha = 1

cv_results <- with(cv_model, 
  data.frame(
  log_lambda = log(lambda),
  mean_mse = cvm,
  lower_mse = cvlo,
  upper_mse = cvup
))

best_lambda <- cv_model$lambda.min
log_best_lambda <- log(best_lambda)
```

$\lambda$ is the regularization parameter in Lasso regression, controlling variable selection and the sparsity of the model. Finding the optimal $\lambda$ value is crucial as it balances model complexity and predictive performance. The ideal $\lambda$ value, derived from cross-validation, minimizes model error. For this study, the optimal $\lambda$ has been determined to be 0.005529689. 

```{r cv_results, message=FALSE, warning=FALSE}

ggplot(cv_results, aes(x = log_lambda, y = mean_mse)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lower_mse, ymax = upper_mse), alpha = 0.2, fill = "blue") +
  geom_vline(xintercept = log_best_lambda, linetype = "dashed", color = "red") +
  labs(
    title = "Cross-Validation for Lasso Regression",
    x = "Log(Lambda)",
    y = "Mean Cross-Validated Error"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

lasso_coefficients <- coef(cv_model, s = "lambda.min")

lasso_coefficients_df <- as.data.frame(as.matrix(lasso_coefficients)) %>%
  rownames_to_column("Variable") %>%
  rename(Coefficient = 1) %>%
  filter(Coefficient != 0)

kable(
  lasso_coefficients_df,
  caption = "Cross-Validation Results for Lasso Regression",
  align = "c",
  digits = 4
) |> 
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = TRUE
  ) |> 
  scroll_box(height = "300px")
```

In summary, the Lasso regression model selected the following variables:

*   Time factor: `year` and `month`
*   Individual characteristics: `gender` and `spay_neuter`
*   Regional characteristics: `borough`

Therefore, these variables need to be included in the subsequent negative binomial regression analysis.

## Main effects model

After employing Lasso regression for feature selection, the study identified several key variables that influence the occurrence of dog bite incidents.   These variables include the `year`, `month`, `gender`, `spay_neuter`, and `borough`. These factors will be incorporated into a negative binomial generalized linear model (`glm.nb`) to predict the number of bite incidents.

### Results

The results of the main effects model fitting are presented in the following table. 

The dispersion parameter, $\theta = 25.18$, indicates that the data exhibits overdispersion, meaning the variance is much larger than the mean. This justifies the use of the negative binomial regression model over a Poisson regression.

The deviance of the null model (a model with no predictors, only the intercept) is $3914.5$. This represents the deviance of the fitted model with predictors included is $1549.7$. The significant reduction in deviance indicates that the included predictors substantially improve model fit.

```{r glm_nb, message=FALSE, warning=FALSE, include=FALSE}
library(MASS)

model = glm.nb(bite_count ~ year + month + gender + spay_neuter + borough, data = bites_by_group)


model_summary <- tidy(model) %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  )
```

```{r}
kable(
  model_summary,
  col.names = c("Term", "Estimate", "SE", "Statistic", "P-value", "Significance"),
  caption = "Summary of Negative Binomial Regression Results"
) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE) %>%
  scroll_box(height = "400px")
```

Year: Some years (e.g., 2017, 2019) show statistically significant coefficients, suggesting that the number of bite incidents varies significantly across years. `year2021` (Coefficient = -0.28710, p < 0.001) indicates a significant *decrease* in bite counts compared to year2015.

Month: Some months show no significant effects on bite counts (e.g., month2: p = 0.97; month3: p = 0.61). Several months (e.g., May, June, August) have significant coefficients, indicating seasonality in bite incidents.

Gender: Male dogs (genderM) are associated with a significantly higher bite count, as indicated by the large positive coefficient.

Spay/Neuter: Neutered dogs are associated with a lower bite count, with a statistically significant negative coefficient.

Boroughs: Bite counts vary across boroughs, with Staten Island showing a significant negative association compared to the reference borough.

From the coefficient of fitting results, this study draws the following conclusions:

* Different years have significantly impacted bite counts, particularly after 2020, where a notable reduction in incidents was observed. This might reflect external factors, such as the COVID-19 pandemic.

* Monthly effects highlight significant seasonal variation, with summer months (e.g., June) showing higher bite counts.

* Males are significantly more likely to be involved in bite incidents than females.

* Neutered animals have significantly lower bite risks compared to non-neutered ones.

* Brooklyn has significantly higher bite counts, while Staten Island has significantly fewer bites compared to Bronx. These differences might reflect variations in dog management policies, population density, or dog ownership practices across regions.

### Evaluation

After obtaining the fitted results, it is crucial to perform a multicollinearity test on the model to ensure the reliability of the regression coefficients. Multicollinearity can severely affect the interpretability of the model and can lead to overinflated standard errors, making it difficult to discern the true effect of each predictor.

```{r vif, warning=FALSE, message=FALSE}
library(car)
vif_results <- vif(model) %>%
  as.data.frame() %>%
  rownames_to_column("Predictor") %>%
  rename(
    GVIF = GVIF,
    DF = Df,
    `GVIF^(1/(2*DF))` = `GVIF^(1/(2*Df))`
  )

kable(
  vif_results,
  col.names = c("Predictor", "GVIF", "DF", "Scaled GVIF"),
  caption = "Multicollinearity Check (GVIF)"
) |> 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE)
```

Generalized Variance Inflation Factor (GVIF) measures the multicollinearity in the regression model. A GVIF value greater than 10 generally indicates a high multicollinearity issue. All GVIF values are close to 1, suggesting no significant multicollinearity.

Scaled GVIF adjusts GVIF based on the degrees of freedom to make it comparable across variables. It is defined as $GVIF^{(1/(2*Df))}$.
As shown in the table above, the GVIF and scaled GVIF are all close to 1, indicating that there is no significant multicollinearity in the final model.

As shown in the table above, results does not exhibit multicollinearity among the predictors, as all GVIF values and their scaled counterparts are well below the threshold of concern (commonly 10 or higher). This indicates that all predictors can be reliably used in regression model without the risk of biased coefficients caused by multicollinearity.

```{r residuals_sim, warning=FALSE, message=FALSE}
library(DHARMa)
residuals_sim = simulateResiduals(model)
plot(residuals_sim)
```

`Q-Q Plot`: 

* KS Test p-value (0.7167): Indicates that the residuals do not significantly deviate from a uniform distribution. This suggests that the model residuals are well-behaved.
* Dispersion Test p-value (0.936): Suggests there is no significant over- or under-dispersion in the model, further supporting the appropriateness of the negative binomial distribution.
* Outlier Test p-value (0.19983): Shows that there are no significant outliers in the residuals, meaning that most predictions align with the observed data.

The residuals closely follow the expected line, indicating a good fit in terms of normality for the main effects-only model.

`Residuals vs Predicted`: 

* The residuals appear to be well-distributed, with most points clustering around the 0.5 quantile line.
* The red curves (quantile deviations) show minor deviations, particularly near the tails (e.g., quantiles close to 0 and 1). However, these deviations are not substantial enough to indicate severe model misspecification.
* The presence of small red asterisks (*) at the upper quantiles suggests slight issues with model fit in extreme predictions, but these deviations are minor.

This shows some spread and potential systematic patterns, indicating the main effects model might *not fully* capture the variability in the data.

## Model with interaction

To further increase the precision of the model fit, interaction terms between variables were added.

### Results

```{r glm_nb_interactions, warning=FALSE, message=FALSE, results = "hide"}
model_all_interactions <- glm.nb(
  formula = bite_count ~ 
    year + month + gender + spay_neuter + borough +
    year * month + year * gender + year * spay_neuter + year * borough +
    month * gender + month * spay_neuter + month * borough +
    gender * spay_neuter + gender * borough + spay_neuter * borough +
    year * month * gender + year * month * spay_neuter + year * month * borough + 
    year * gender * spay_neuter + year * gender * borough + year * spay_neuter * borough +
    month * gender * spay_neuter + month * gender * borough + month * spay_neuter * borough +
    gender * spay_neuter * borough,
  data = bites_by_group
)

model_stepwise <- stepAIC(model_all_interactions, direction = "both")
```

After using stepwise reduction to simplify the model complexity, the variables of final negative binomial regression model are as follow:

```{r final_NBR, message=FALSE, warning=FALSE}
data <- data.frame(
  `Main Effect` = c("year", "month", "gender", "spay_neuter", "borough", " "),
  `Double Interaction Effect` = c(
    "year * spay_neuter",
    "year * borough",
    "month * spay_neuter",
    "gender * spay_neuter",
    "gender * borough",
    "spay_neuter * borough"
  )
)

knitr::kable(data, format = "pipe", align = "l", caption = "Final Negative Binomial Regression Model", escape = FALSE)
```

The final model with interaction is detailed in the following table. Initially, the intercept is 3.83 with a p-value near zero, indicating that the baseline level of the response variable (on a logarithmic scale) is significant when other variables are not considered. For the main effect variables, the years 2016, 2018, and 2019 show a significant increase in the log change of event counts compared to 2015; male dogs, compared to female dogs, exhibit stronger aggression, with a 10.59 times increase in the incidence of dog bite events, calculated as with $e^{2.36}$. The coefficient for neutering, 0.3960, indicates a significant reduction in the log count of bite incidents for neutered dogs compared to unneutered dogs. Geographically, the coefficients for Manhattan and Staten Island are 0.72 and 0.48, respectively, showing significantly lower event counts compared to the Bronx. Regarding interaction terms, the years 2018, 2020, and 2021 show a significant reduction in interaction with the neutering status.

```{r final_interaction, message=FALSE, warning=FALSE}
filter_model = glm.nb(bite_count ~ year + month + gender + spay_neuter + 
    borough + year:spay_neuter + year:borough + month:spay_neuter + 
    gender:spay_neuter + gender:borough + spay_neuter:borough, 
    data = bites_by_group)

filter_model_summary <- tidy(filter_model, exponentiate = TRUE, conf.int = TRUE)

kable(
  filter_model_summary,
  caption = "Summary of Negative Binomial Regression (with interactions) Results"
) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE) %>%
  scroll_box(height = "400px")
```

### Evaluation

Additionally, the quality of the fitted model can be assessed through residual analysis and diagnostics, as illustrated in the following four plots.

*  Residual vs. Fitted Plot: 

This graph helps to detect non-linearity, unequal error variances, and outliers. Ideally, the residuals should be randomly dispersed around the horizontal axis, meaning the red dashed line (a smooth curve) should be close to a horizontal straight line.

*   Q-Q Plot: 

This plot is used to check whether the residuals conform to the distribution assumed by the model.

*   Scale-Location Plot: 

This plot shows the spread of residuals versus fitted values and helps to check for homoscedasticity (constant spread of residuals). A model that meets this assumption will show a horizontal line with randomly spread points.

*   Residuals vs. Leverage Plot:

This plot is critical for identifying observations with high leverage values—points that have a significant impact on the model's fit. Ideally, no points should exceed the threshold set by Cook’s distance, indicated by the dashed lines.

```{r, warning=FALSE, message=FALSE}
library(DHARMa)
residuals_sim = simulateResiduals(model_stepwise)
plot(model_stepwise)
```

From the analysis of the four diagnostic plots, the fitted model exhibits several shortcomings. Firstly, the tails of the residual distribution slightly deviate from the assumed negative binomial distribution. This deviation suggests that the residuals might not be perfectly modeled, indicating potential issues with the model fit or the distributional assumptions. Secondly, the distribution of points in the Scale-Location plot appears striated, and the red smooth line shows a non-horizontal trend. This pattern indicates that the variance of the residuals may change with fitted values, suggesting the presence of heteroscedasticity. This non-constant variance can affect the reliability of the model's standard errors and confidence intervals.Lastly, the Residuals vs Leverage Plot reveals the presence of points with high leverage, which could be overly influencing the model's predictions. These high leverage points warrant further investigation to determine whether they should be removed or adjusted to improve model accuracy and robustness.

### Prediction

Cross-validation of the final model is depicted in the following illustration, where the red dashed line represents the ideal prediction scenario (i.e., predicted values equal actual values). If points fall near this line, it indicates that the model predicts well. Points located above the diagonal line suggest that the model underestimates the actual values; conversely, if points are below the diagonal line, the model overestimates the actual values. 

```{r, warning=FALSE, message=FALSE}
# Predicted vs Actual Bite Count
inter_predict = bites_by_group |> 
  mutate(predicted = predict(model_stepwise, type = "response"))

ggplot(inter_predict, aes(x = predicted, y = bite_count)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Bite Count",
       x = "Predicted Bite Count",
       y = "Actual Bite Count") +
  theme_minimal()
```

Overall, while the scatter plot shows a slight systematic deviation, the final negative binomial regression model still accurately predicts low and medium value areas. However, the error is larger in high-value areas, where actual values tend to fall below the predicted values. This may indicate a tendency of the model to overfit in regions with high incident counts.

# Generalized Linear Mixed Model (GLMM)

To address the model fitting issue, a generalized linear mixed model is employed. 
The Generalized Linear Mixed Model (GLMM) is an extension of the Generalized Linear Model (GLM) and the Linear Mixed Model (LMM). GLMM consists of three main components: random effects, fixed effects, and a link function. Fixed effects represent the overall level parameters consistent across all observations, while random effects account for individual variability or group-level differences that may influence the response variable. The link function connects the linear predictor to the mean of the response variable distribution, allowing the dependent variable to deviate from a normal distribution and enabling modeling of various types of data, including binary, count, and continuous outcomes.

Let the response variable $y$ belongs to the exponential family, its conditional distribution can be expressed as follows:

$$f(y|b) = \prod\limits_{i = 1}^n {f({y_i}|{\eta _i})}$$

Where ${\eta _i} = {x_i}^ \top \beta  + {z_i}^ \top b$. Additionally, the random effect $b$ is assumed to be distributed according to $b \sim {\rm N}(0,\Sigma )$.

Therefore the GLMM is structured as follows:

$$g(\vec {\rm E}(y)) = {\rm X}\vec \beta  + {\rm Z}\vec b + \varepsilon$$

In the model, $g( \cdot )$ represents the link function, which connects the expected value of the response variable to the linear predictor. Here, the logarithm function $g(\mu ) = \ln (\mu )$ is used, which is appropriate for the Poisson distribution. $\vec{\mathrm{E}}(y)$ denotes the vector of expected values for the response variable, and $\varepsilon$ represents the error term, capturing random noise or the unexplained variance. ${\rm X}\vec \beta  + {\rm Z}\vec b$ represents the linear combination of fixed and random effects, ${\rm X}\vec \beta$ is used to explain global trends, and ${\rm Z}\vec b$ is utilized to capture differences among observations. In this context, ${\rm X}$ is the design matrix for fixed effects, which is of order $n \times p$, where $n$ is the number of observations and $p$ is the number of fixed effect variables; $\vec \beta$ is the vector of coefficients for fixed effects, with each element representing the regression coefficient of a fixed effect variable. Similarly, ${\rm Z}$ also serves as the design matrix for random effects, of order $n \times q$, where $q$ is the number of random effect variables. $\vec b$ is the vector of coefficients for random effects, representing inter-group deviations used to explain random differences between groups. The model assumes $b \sim {\rm N}(0,\Sigma )$, thereby the random effects are normally distributed with a mean of $0$ and a covariance matrix $\Sigma$.

For the dog bite incidents dataset, the response variable $y$ represents the number of dog bite incidents per zipcode per month. Fixed effects in the model include year, month, gender, and sterilization status. Random effects are attributed to different boroughs (5 boroughs in total). Thus, for all response variables, ${\rm X}$ is a matrix containing year, month, gender, and sterilization status; $\vec \beta$ represents the coefficients for the fixed effects; ${\rm X}$ indicates the matrix for the borough each response variable belongs to; and $\vec b$ denotes the deviation for each borough (random effects). This setup allows the model to adjust for the influence of both environmental conditions and borough-specific variations in the occurrence of dog bites.

## Results

```{r glmer_nb, warning=FALSE, message=FALSE}
library(lme4)
glmm_model = glmer.nb(
  bite_count ~ year + month + gender + spay_neuter + (1 | borough), 
  data = bites_by_group
)
summary(glmm_model)

library(broom.mixed)
glmm_model_summary <-
  broom.mixed::tidy(glmm_model) %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  ) |> 
  dplyr::select(-effect, -group)
```

```{r}
kable(
  glmm_model_summary,
  col.names = c("Term", "Estimate", "SE", "Statistic", "P-value", "Significance"),
  caption = "Summary of Negative Binomial Regression Results"
) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE) %>%
  scroll_box(height = "400px")
```

Random Effects：

`Borough`: Variance: 0.1297, standard Deviation: 0.3601. Indicates moderate variability between boroughs in the baseline bite count.

Fixed Effects:

* `Year`: 2017 has *positive* coefficient (0.15336, p < 0.001), suggesting a higher bite count compared to 2015. While 2020 and 2021 have strong *negative* coefficients (-0.27884 and -0.28669, p < 0.001), indicating a substantial reduction in bite counts. This could relate to external factors like the COVID-19 pandemic.
* `Month`: Positive coefficient (0.12734, p = 0.026), suggesting higher bite counts in April. In May, June and July, all p < 0.001, maybe there is a seasonal peak during summer.
* `Gender`: Male dogs have strong positive coefficient (0.88482, p < 0.001), indicating male dogs have a significantly higher bite count compared to female dogs.
* `Spay/Neuter Status`: Dogs who are neutered have have significantly lower bite counts (-0.11231, p < 0.001).

## Evaluation

```{r, warning=FALSE, message=FALSE}
library(DHARMa)
residuals_glmm_sim = simulateResiduals(glmm_model)
plot(residuals_glmm_sim)
```

`Q-Q Plot`:

* Kolmogorov-Smirnov (KS) Test: p-value = 0.05845: Indicates that the residual distribution is not significantly different from the expected distribution (n.s. means not significant). The model fits the overall data distribution well.
* Dispersion Test: p-value = 0.896: Suggests no evidence of overdispersion or underdispersion in the residuals. This confirms the appropriateness of using the negative binomial model for handling count data with dispersion.
* Outlier Test: p-value = 0.00056: Indicates significant deviations due to outliers. This suggests that there are specific data points that the model fails to fit well.

`Residuals vs. Predicted`:

* The red lines (quantile deviations) highlight significant deviations at certain ranges of predictions, particularly near the lower predicted values (around the 0.25 quantile). This indicates a slight mismatch between the model predictions and observed data for certain ranges.
* Despite the overall good fit, systematic patterns (e.g., slight curvature) in the residuals suggest possible room for improvement in model specification.

# Model comparison

三者相比，AIC。。。
并且，残差分析显示，只有with interaction的有意义。

Comparison to the Previous Model

主效应 vs glmm：

The KS test and dispersion test results in both models are similar, suggesting both models handle overall distribution and dispersion well.

However, the outlier test is significant in `glmm model`, whlie the outlier test is insignificant in the `model`, indicating that this alternative model has difficulty capturing certain extreme values or specific subsets of the data.

# Conclusion

三个模型对比，只含有主效应的model。。。。
含有inter。。。
后续生成的。。。但是能解释一些。。。

